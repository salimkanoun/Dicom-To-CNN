{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2,
  "kernelspec": {
   "name": "python38564bit9a8409779cd04c3cbd0c5f1e859a644e",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import shutil\n",
    "from collections import defaultdict, Counter\n",
    "from library_dicom.dicom_processor.tools.folders import write_json_file\n",
    "from library_dicom.dicom_processor.tools.cleaning_series import *"
   ]
  },
  {
   "source": [
    "merged_content = generate_merged_file('/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/relevance_json_2')\n",
    "write_json_file('/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/RELEVANCE_json', 'merged_files', merged_content)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "series_ready = find_studies_with_two_series(\"/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/RELEVANCE_json/merged_files.json\")\n",
    "print(len(series_ready))"
   ]
  },
  {
   "source": [
    "studies_over_two_series, paths = find_studies_over_two_series(\"/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/RELEVANCE_json/merged_files.json\")\n",
    "print(len(studies_over_two_series))\n",
    "print(Counter(studies_over_two_series))"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " LOR-RAMLA': 1, 'p16450s0_wb_ctac.img: LOR-RAMLA': 1, 'p16450s0_wb_nac.img: 3D-RAMLA': 1, 'PET TAP NON HD (AC)': 1, 'p11861s1_wb_nac.img: 3D-RAMLA': 1, 'p11861s1_wb_ctac.img: LOR-RAMLA': 1, 'CT CORPS ENTIER 2.5MM': 1, 'CT 1.25MM': 1, 'TEP CE AC': 1, 'p12713s0_wb_nac.img: 3D-RAMLA': 1, 'p12713s0_wb_ctac.img: LOR-RAMLA': 1, 'p12713s1_wb_ctac.img: LOR-RAMLA': 1, 'p12713s1_wb_nac.img: 3D-RAMLA': 1, 'WB FDG QC400 2min': 1, 'AC CT': 1, 'PET TAP Corrected HD TOF': 1, 'CT TAP  3.0  B20f': 1, 'p16794s0_wb_nac.img: 3D-RAMLA': 1, 'p16794s0_wb_ctac.img: LOR-RAMLA': 1, 'p16794s1_wb_ctac.img: LOR-RAMLA': 1, 'p16794s1_wb_nac.img: 3D-RAMLA': 1, 'p13499s1_wb_ctac.img: LOR-RAMLA': 1, 'p13499s1_wb_nac.img: 3D-RAMLA': 1, 'p13334s0_wb_nac.img: 3D-RAMLA': 1, 'p13334s0_wb_ctac.img: LOR-RAMLA': 1, 'CT ORL  2.5  B30s': 1, 'CT TAP 3.0  B30': 1, 'PET ORL AC HD': 1, 'p13803s1_wb_nac.img: 3D-RAMLA': 1, 'p13803s1_wb_ctac.img: LOR-RAMLA': 1, 'p11970s3_wb_nac.img: 3D-RAMLA': 1, 'p11970s3_wb_ctac.img: LOR-RAMLA': 1})\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "405\n"
     ]
    }
   ],
   "source": [
    "print(len(studies_over_two_series))"
   ]
  },
  {
   "source": [
    "export_folder = '/media/deeplearning/Elements/check_serie'\n",
    "for path in paths : \n",
    "    try : \n",
    "        #print(path)\n",
    "        dest = '/'.join(path.split('/')[5:])\n",
    "        #print(dest)\n",
    "        destination = export_folder+'/'+dest\n",
    "        shutil.copytree(path, destination)\n",
    "    except Exception as err : \n",
    "        print(err)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "7.4869.2.564.0.1430121185'\n",
      "[Errno 2] No such file or directory: '/media/deeplearning/Elements/RELEVANCE/12011101191108/PET_24weeks/1.3.46.670589.28.2.10.4.9182.45731.2.1116.0.1418732296'\n",
      "[Errno 2] No such file or directory: '/media/deeplearning/Elements/RELEVANCE/12011101191110/PET_Screening/1.3.46.670589.28.2.10.7.59695.20379.2.1976.0.1405596736'\n",
      "[Errno 2] No such file or directory: '/media/deeplearning/Elements/RELEVANCE/12011101061106/PET_24weeks/1.3.12.2.1107.5.1.4.45527.30000015032416493623400000413'\n",
      "[Errno 2] No such file or directory: '/media/deeplearning/Elements/RELEVANCE/12011101061106/PET_24weeks/1.3.12.2.1107.5.1.4.45527.30000015032416504662500000834'\n",
      "[Errno 2] No such file or directory: '/media/deeplearning/Elements/RELEVANCE/12011101031106/PET_Screening/1.2.840.113704.7.32.089.28.2.10.4.9167.27815.2.1540.0.1357651971'\n",
      "[Errno 2] No such file or directory: '/media/deeplearning/Elements/RELEVANCE/12011101191105/PET_pwd/1.3.46.670589.28.2.10.7.59695.20379.2.556.0.1432205509'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "108\n"
     ]
    }
   ],
   "source": [
    "non_interesting_series  = find_non_intersting_series(\"/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/RELEVANCE_json/merged_files.json\")\n",
    "print(len(non_interesting_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for non_interisting in non_interesting_series:\n",
    "    try : \n",
    "        shutil.rmtree(non_interisting)\n",
    "    except Exception as err : \n",
    "        print(err)"
   ]
  },
  {
   "source": [
    "root_destination = \"/media/deeplearning/Elements/RELEVANCE_Validated_DICOM\"\n",
    "for seriesID in series_ready : \n",
    "    try : \n",
    "        source_path = series_ready[seriesID]['path']\n",
    "        parentPatientID = series_ready[seriesID]['parentPatientID']\n",
    "        parentStudyUID = series_ready[seriesID]['parentStudyUID']\n",
    "        destination = root_destination+\"/\"+parentPatientID+\"/\"+parentStudyUID+\"/\"+seriesID\n",
    "        os.makedirs(destination, exist_ok=True)\n",
    "        shutil.move(source_path, destination)\n",
    "    except Exception as err : \n",
    "        print(err)"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "262\n"
     ]
    }
   ],
   "source": [
    "#GOAL : Find study name and write it into a csv file \n",
    "\n",
    "liste_json = os.listdir('/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/relevance_json_2')\n",
    "print(len(liste_json))\n",
    "for i in range(len(liste_json)): \n",
    "    liste_json[i] = os.path.join('/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/relevance_json_2', liste_json[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get study_uid from folder Validated Dicom \n",
    "from library_dicom.dicom_processor.tools.folders import *\n",
    "series_paths = get_series_path(\"/media/deeplearning/Elements/RELEVANCE_Validated_DICOM\")\n",
    "study_uid_liste = []\n",
    "for serie_path in series_paths : \n",
    "    study_uid = serie_path.split('/')[6]\n",
    "    study_uid_liste.append(study_uid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep one study uid for 2 serie \n",
    "\n",
    "l = []\n",
    "l.append(study_uid_liste[0])\n",
    "for uid in study_uid_liste : \n",
    "    if uid not in l : \n",
    "        l.append(uid)\n",
    "\n",
    "study_uid_liste = l \n",
    "\n",
    "liste = []\n",
    "for uid in study_uid_liste : \n",
    "    subliste = []\n",
    "    for json_ in liste_json : \n",
    "        with open (json_) as json_file : \n",
    "            reader = json.load(json_file)\n",
    "            data = []\n",
    "\n",
    "            for info in reader['study']['StudyInstanceUID'] : \n",
    "                data.append(info)\n",
    "                study_uid_json = \"\".join(data)\n",
    "\n",
    "                if uid == study_uid_json : \n",
    "                    subliste.append(json_)\n",
    "    subliste.append(uid)\n",
    "\n",
    "    liste.append(subliste)\n",
    "\n",
    "#if more than 2 series in the study \n",
    "error = []\n",
    "for serie in liste : \n",
    "    if len(serie) != 3 : \n",
    "        #print(serie)\n",
    "        error.append(serie)\n",
    "\n",
    "for err in error : \n",
    "    liste.remove(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean study with more than 2 series \n",
    "new = []\n",
    "for err in error : \n",
    "    subliste = []\n",
    "    for serie in series_paths : \n",
    "        serie_uid = serie.split('/')[-1] \n",
    "\n",
    "        for i in range(len(err)):\n",
    "            if serie_uid+'.json' in err[i] : \n",
    "                subliste.append(err[i])\n",
    "\n",
    "    subliste.append(err[-1])\n",
    "\n",
    "    new.append(subliste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "#verify if all the study has only 2 series, len(check) must be 0\n",
    "check = []\n",
    "for serie in new : \n",
    "    if len(serie) != 3 :\n",
    "        check.append(serie)\n",
    "print(check)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = liste + new \n",
    "#[[json 1, json_2, study_uid ], ... ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/relevance_json_2/1.3.46.670589.28.2.10.48.18551.54540.2.3680.0.1436269329.json',\n",
       " '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/relevance_json_2/1.2.840.113704.1.111.4532.1436267350.8.json',\n",
       " '1.2.840.113704.1.111.2748.1436267005.19']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##Get patientID and name of the study \n",
    "\n",
    "for study in dataset : \n",
    "    with open(study[0]) as json_file : \n",
    "        reader = json.load(json_file)\n",
    "        data = []\n",
    "        patient_id = []\n",
    "        for info in reader['path'] : \n",
    "            data.append(info)\n",
    "            path = \"\".join(data)\n",
    "\n",
    "        for info in reader['patient']['PatientID'] : \n",
    "            patient_id.append(info)\n",
    "            pat_id = \"\".join(patient_id)\n",
    "        \n",
    "        name = path.split('/')[-2]\n",
    "\n",
    "    study.append(name)\n",
    "    study.append(pat_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get path, json, and informations about each study \n",
    "data = []\n",
    "for serie in dataset: \n",
    "    subliste = []\n",
    "    serie_uid = []\n",
    "    for i in range(2) : \n",
    "        serie_uid.append(serie[i].split('/')[-1][:-5])\n",
    "\n",
    "    for uid in serie_uid : \n",
    "        for serie_path in series_paths : \n",
    "            if uid in serie_path : \n",
    "                subliste.append(serie_path)\n",
    "    subliste.append(serie[0])\n",
    "    subliste.append(serie_uid[0])\n",
    "    subliste.append(serie[1])\n",
    "    subliste.append(serie_uid[1])\n",
    "\n",
    "    subliste.append(serie[2])\n",
    "    subliste.append(serie[3])\n",
    "    subliste.append(serie[4])\n",
    "    data.append(subliste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/media/deeplearning/Elements/RELEVANCE_Validated_DICOM/12011101031107/1.2.840.113704.1.111.2748.1436267005.19/1.3.46.670589.28.2.10.48.18551.54540.2.3680.0.1436269329/1.3.46.670589.28.2.10.48.18551.54540.2.3680.0.1436269329',\n",
       " '/media/deeplearning/Elements/RELEVANCE_Validated_DICOM/12011101031107/1.2.840.113704.1.111.2748.1436267005.19/1.2.840.113704.1.111.4532.1436267350.8/1.2.840.113704.1.111.4532.1436267350.8',\n",
       " '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/relevance_json_2/1.3.46.670589.28.2.10.48.18551.54540.2.3680.0.1436269329.json',\n",
       " '1.3.46.670589.28.2.10.48.18551.54540.2.3680.0.1436269329',\n",
       " '/media/deeplearning/78ca2911-9e9f-4f78-b80a-848024b95f92/relevance_json_2/1.2.840.113704.1.111.4532.1436267350.8.json',\n",
       " '1.2.840.113704.1.111.4532.1436267350.8',\n",
       " '1.2.840.113704.1.111.2748.1436267005.19',\n",
       " 'PET_120weeks',\n",
       " '12011101031107']"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "source": [
    "import os \n",
    "import shutil \n",
    "\n",
    "root = '/media/deeplearning/Elements/RELEVANCE_Valid_DICOM'\n",
    "for serie in data : \n",
    "    try : \n",
    "        folder = root+'/'+serie[-1]\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "\n",
    "        study = folder + '/' + serie[-2] \n",
    "        if not os.path.exists(study) : \n",
    "            os.makedirs(study)\n",
    "\n",
    "        for i in range(2) : \n",
    "            if i == 0 : \n",
    "                filename = serie[3]\n",
    "            elif i == 1 : \n",
    "                filename = serie[5]\n",
    "            dest = study+'/'+filename\n",
    "            if not os.path.exists(dest):\n",
    "                os.makedirs(dest)\n",
    "            shutil.move(serie[i], dest)\n",
    "    except Exception as err : \n",
    "        print(err)\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 29,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write a csv with all of information about studies\n",
    "\n",
    "import csv \n",
    "csv_path = '/media/deeplearning/Elements/RELEVANCE_Valid_DICOM/relevance_data.csv'\n",
    "\n",
    "with open(csv_path, 'r') as csv_file :\n",
    "    reader = csv.reader(csv_file, delimiter = ',') \n",
    "    csv_data = []\n",
    "    for row in reader :\n",
    "        csv_data.append(row)\n",
    "        \n",
    "del csv_data[0] \n",
    "\n",
    "\n",
    "nifti_directory = \"/media/deeplearning/Elements/RELEVANCE_Valid_DICOM\"\n",
    "filename = 'relevance_data_v2.csv'\n",
    "with open(os.path.join(nifti_directory, filename), 'w') as csv_file : \n",
    "    csv_writer = csv.writer(csv_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    csv_writer.writerow([\"PATIENT ID\", \"STUDY UID\", \"NAME\", \"SERIE_1\", \"JSON_SERIE_1\", \"SERIE_2\", \"JSON_SERIE_2\"])\n",
    "    for serie in csv_data : \n",
    "        csv_writer.writerow([serie[0], serie[1], serie[2], serie[3], serie[4], serie[5], serie[6]])\n",
    "\n",
    "    for serie in data: \n",
    "        csv_writer.writerow([serie[-1], serie[-3], serie[-2], serie[0], serie[2], serie[1], serie[4]])"
   ]
  }
 ]
}